{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hRpsy5Zp8Lr"
   },
   "source": [
    "\n",
    "## <font color=red> You should not import any new libraries. Your code should run with python=3.x</font>\n",
    "\n",
    "#### <font color=red>For lab assignment, you will work with two datasets. The trained weights need to be saved and shared with us in a folder called models with the name ./models/{dataset_name}_weights.pkl. Your predict function should load these weights, initialize the DNN and predict the labels.</font>\n",
    "\n",
    "- Your solutions will be auto-graded. Hence we request you to follow the instructions.\n",
    "- Modify the code only between \n",
    "```\n",
    "## TODO\n",
    "## END TODO\n",
    "```\n",
    "- In addition to above changes, you can play with arguments to the functions for generating plots\n",
    "- We will run the auto grading scripts with private test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mBBZWQn3WjsN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import pickle as pkl\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9j3in3odIle"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "iaYuScGvdEum"
   },
   "outputs": [],
   "source": [
    "def preprocessing(X):\n",
    "  \"\"\"\n",
    "  Implement Normalization for input image features\n",
    "\n",
    "  Args:\n",
    "  X : numpy array of shape (n_samples, 784)\n",
    "   \n",
    "  Returns:\n",
    "  X_out: numpy array of shape (n_samples, 784) after normalization\n",
    "  \"\"\"\n",
    "  X_out = None\n",
    "  \n",
    "  ## TODO\n",
    "    \n",
    "  X_out = (X - np.mean(X, axis=0))/(np.std(X, axis=0) + 1e-9)\n",
    "  \n",
    "  ## END TODO\n",
    "\n",
    "  assert X_out.shape == X.shape\n",
    "\n",
    "  return X_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejROq-52YUol"
   },
   "source": [
    "### Split data into train/val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "l07sJgZ3XG-N"
   },
   "outputs": [],
   "source": [
    "def split_data(X, Y, train_ratio=0.8):\n",
    "    '''\n",
    "    Split data into train and validation sets\n",
    "    The first floor(train_ratio*n_sample) samples form the train set\n",
    "    and the remaining the test set\n",
    "\n",
    "    Args:\n",
    "    X - numpy array of shape (n_samples, n_features)\n",
    "    Y - numpy array of shape (n_samples, 1)\n",
    "    train_ratio - fraction of samples to be used as training data\n",
    "\n",
    "    Returns:\n",
    "    X_train, Y_train, X_val, Y_val\n",
    "    '''\n",
    "    # Try Normalization and scaling and store it in X_transformed\n",
    "    X_transformed = X\n",
    "\n",
    "    ## TODO\n",
    "    \n",
    "    X_transformed = preprocessing(X)\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "    assert X_transformed.shape == X.shape\n",
    "\n",
    "    num_samples = len(X)\n",
    "    indices = np.arange(num_samples)\n",
    "    num_train_samples = math.floor(num_samples * train_ratio)\n",
    "    train_indices = np.random.choice(indices, num_train_samples, replace=False)\n",
    "    val_indices = list(set(indices) - set(train_indices))\n",
    "    X_train, Y_train, X_val, Y_val = X_transformed[train_indices], Y[train_indices], X_transformed[val_indices], Y[val_indices]\n",
    "  \n",
    "    return X_train, Y_train, X_val, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FujjCCbMbsu4"
   },
   "source": [
    "#Flatten the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hl8LxP1lAEiN"
   },
   "outputs": [],
   "source": [
    "class FlattenLayer:\n",
    "    '''\n",
    "    This class converts a multi-dimensional into 1-d vector\n",
    "    '''\n",
    "    def __init__(self, input_shape):\n",
    "        \n",
    "         '''\n",
    "         Args:\n",
    "          input_shape : Original shape, tuple of ints\n",
    "         '''\n",
    "         self.input_shape = input_shape\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Converts a multi-dimensional into 1-d vector\n",
    "        Args:\n",
    "          input : training data, numpy array of shape (n_samples , self.input_shape)\n",
    "\n",
    "        Returns:\n",
    "          input: training data, numpy array of shape (n_samples , -1)\n",
    "        '''\n",
    "        ## TODO\n",
    "        #Modify the return statement to return flattened input\n",
    "        self.input = input.flatten()\n",
    "        self.input = self.input.reshape(1, self.input.shape[0])\n",
    "        return self.input\n",
    "        \n",
    "        ## END TODO\n",
    "        \n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Converts back the passed array to original dimention \n",
    "        Args:\n",
    "        output_error :  numpy array \n",
    "        learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "        output_error: A reshaped numpy array to allow backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        #Modify the return statement to return reshaped array\n",
    "        return output_error.reshape(self.input_shape)\n",
    "        ## END TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02MOHEdgh7T6"
   },
   "source": [
    "#Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "oTrTMpTwtLXd"
   },
   "outputs": [],
   "source": [
    "class FCLayer:\n",
    "    '''\n",
    "    Implements a fully connected layer  \n",
    "    '''\n",
    "    def __init__(self, input_size, output_size):\n",
    "        '''\n",
    "        Args:\n",
    "         input_size : Input shape, int\n",
    "         output_size: Output shape, int \n",
    "        '''\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        ## TODO\n",
    "        #initialize weights and biases\n",
    "        self.weights = np.random.randn(self.input_size, self.output_size)*np.sqrt(2/(self.input_size + self.output_size))\n",
    "        self.bias = np.random.randn(1, self.output_size)\n",
    "        ## END TODO\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Performs a forward pass of a fully connected network\n",
    "        Args:\n",
    "          input : training data, numpy array of shape (n_samples , self.input_size)\n",
    "\n",
    "        Returns:\n",
    "           numpy array of shape (n_samples , self.output_size)\n",
    "        '''\n",
    "        ## TODO\n",
    "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
    "        self.inputs = input\n",
    "        return np.matmul(input, self.weights) + self.bias\n",
    "        ## END TODO\n",
    "        \n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter \n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "\n",
    "        #Modify the return statement to return numpy array resulting from backward pass\n",
    "        dw = np.matmul(self.inputs.T, output_error)\n",
    "        db = np.sum(output_error, axis=0)\n",
    "        self.weights -= learning_rate*dw\n",
    "        self.bias -= learning_rate*db\n",
    "        return np.matmul(output_error, self.weights.T)\n",
    "\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "E6nSYAB2sam3"
   },
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    '''\n",
    "    Implements a Activation layer which applies activation function on the inputs. \n",
    "    '''\n",
    "    def __init__(self, activation, activation_prime):\n",
    "         '''\n",
    "          Args:\n",
    "          activation : Name of the activation function (sigmoid,tanh or relu)\n",
    "          activation_prime: Name of the corresponding function to be used during backpropagation (sigmoid_prime,tanh_prime or relu_prime)\n",
    "         '''\n",
    "         self.activation = activation\n",
    "         self.activation_prime = activation_prime\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Applies the activation function \n",
    "        Args:\n",
    "          input : numpy array on which activation function is to be applied\n",
    "\n",
    "        Returns:\n",
    "           numpy array output from the activation function\n",
    "        '''\n",
    "        ## TODO\n",
    "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
    "        self.inputs = input\n",
    "        return self.activation(input)\n",
    "        ## END TODO\n",
    "        \n",
    "\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a fully connected network along with updating the parameter \n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "        #Modify the return statement to return numpy array resulting from backward pass\n",
    "        return np.multiply(output_error,self.activation_prime(self.inputs))\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RQeuIfkK3vyl"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SoftmaxLayer:\n",
    "    '''\n",
    "      Implements a Softmax layer which applies softmax function on the inputs. \n",
    "    '''\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "    \n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Applies the softmax function \n",
    "        Args:\n",
    "          input : numpy array on which softmax function is to be applied\n",
    "\n",
    "        Returns:\n",
    "           numpy array output from the softmax function\n",
    "        '''\n",
    "        ## TODO\n",
    "\n",
    "        #Modify the return statement to return numpy array of shape (n_samples , self.output_size)\n",
    "        self.inputs = input\n",
    "        self.outputs = np.exp(input)/(np.sum(np.exp(input))+1e-12)\n",
    "        return self.outputs\n",
    "        ## END TODO\n",
    "        \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        '''\n",
    "        Performs a backward pass of a Softmax layer\n",
    "        Args:\n",
    "          output_error :  numpy array \n",
    "          learning_rate: float\n",
    "\n",
    "        Returns:\n",
    "          Numpy array resulting from the backward pass\n",
    "        '''\n",
    "        ## TODO\n",
    "\n",
    "        #Modify the return statement to return numpy array resulting from backward pass\n",
    "        #back propagation for softmax layer\n",
    "        #jacobian matrix for softmax\n",
    "        \n",
    "        jacobian = np.diagflat(self.outputs) - np.matmul(self.outputs.T, self.outputs)\n",
    "        res = np.matmul(jacobian, output_error.T)\n",
    "        return res.T\n",
    "        ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LuPbn70Wt8Q7"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    '''\n",
    "    Sigmoid function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying simoid function\n",
    "    '''\n",
    "    ## TODO\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return 1/(1 + np.exp(-x))\n",
    "    ## END TODO\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    '''\n",
    "     Implements derivative of Sigmoid function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of Sigmoid function\n",
    "    '''\n",
    "    ## TODO\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return sigmoid(x)*(1-sigmoid(x))\n",
    "    ## END TODO\n",
    "\n",
    "def tanh(x):\n",
    "    '''\n",
    "    Tanh function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying tanh function\n",
    "    '''\n",
    "    ## TODO\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return np.tanh(x)\n",
    "    ## END TODO\n",
    "\n",
    "def tanh_prime(x):\n",
    "    '''\n",
    "     Implements derivative of Tanh function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of Tanh function\n",
    "    '''\n",
    "    ## TODO\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return 1 - np.square(np.tanh(x))\n",
    "    ## END TODO\n",
    "\n",
    "def relu(x):\n",
    "    '''\n",
    "    ReLU function \n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying ReLU function\n",
    "    '''\n",
    "    ## TODO\n",
    "\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return np.maximum(0, x)\n",
    "    ## END TODO\n",
    "\n",
    "def relu_prime(x):\n",
    "    '''\n",
    "     Implements derivative of ReLU function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of ReLU function\n",
    "    '''\n",
    "    ## TODO\n",
    "\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return np.where(x > 0, 1, 0)\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "rXY7jkUzuqEk"
   },
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    '''\n",
    "    MSE loss\n",
    "    Args:\n",
    "        y_true :  Ground truth labels, numpy array \n",
    "        y_true :  Predicted labels, numpy array \n",
    "    Returns:\n",
    "       loss : float\n",
    "    '''\n",
    "    ## TODO\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return np.mean(np.square(y_true - y_pred))\n",
    "    ## END TODO\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    '''\n",
    "    Implements derivative of MSE function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of MSE function\n",
    "    '''\n",
    "    ## TODO\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return 2*(y_true - y_pred)/y_true.shape[1]\n",
    "    ## END TODO\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    '''\n",
    "    Cross entropy loss \n",
    "    Args:\n",
    "        y_true :  Ground truth labels, numpy array \n",
    "        y_true :  Predicted labels, numpy array \n",
    "    Returns:\n",
    "       loss : float\n",
    "    '''\n",
    "    ## TODO\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    # return cross entropy loss\n",
    "    return -1*np.mean(np.sum(np.multiply(y_true, np.log(y_pred)), axis=1))\n",
    "    \n",
    "    ## END TODO\n",
    "\n",
    "def cross_entropy_prime(y_true, y_pred):\n",
    "    '''\n",
    "    Implements derivative of cross entropy function, for the backward pass\n",
    "    Args:\n",
    "        x :  numpy array \n",
    "    Returns:\n",
    "        Numpy array after applying derivative of cross entropy function\n",
    "    '''\n",
    "    ## TODO\n",
    "\n",
    "    #Modify the return statement to return numpy array resulting from backward pass\n",
    "    return -y_true/y_pred\n",
    "    ## END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u23euUDztNtb"
   },
   "source": [
    "Fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-sCYdGN8tSdp"
   },
   "outputs": [],
   "source": [
    "def fit(X_train, Y_train,dataset_name):\n",
    "\n",
    "    '''\n",
    "    Create and trains a feedforward network\n",
    "\n",
    "    Do not forget to save the final weights of the feed forward network to a file. Use these weights in the `predict` function \n",
    "    Args:\n",
    "        X_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        Y_train -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "        dataset_name -- name of the dataset (flowers or mnist)\n",
    "    \n",
    "    '''\n",
    "     \n",
    "    #Note that this just a template to help you create your own feed forward network \n",
    "    ## TODO\n",
    "\n",
    "    #define your network\n",
    "    #This network would work only for mnist\n",
    "\n",
    "    network = [\n",
    "        FlattenLayer(input_shape=(28, 28)),\n",
    "        FCLayer(28*28, 12),\n",
    "        ActivationLayer(tanh, tanh_prime),\n",
    "        FCLayer(12, 10),\n",
    "        SoftmaxLayer(10)\n",
    "    ] # This creates feed forward \n",
    "\n",
    "\n",
    "    # Choose appropriate learning rate and no. of epoch\n",
    "    epochs = 50\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    #implement one hot encoding of Y_train\n",
    "\n",
    "    #implement batch gradient descent\n",
    "    for epoch in range(epochs):\n",
    "        error = 0\n",
    "        acc = 0\n",
    "        for x, y_true in zip(X_train, Y_train):\n",
    "            # forward\n",
    "            y_onehot = np.eye(10)[y_true]\n",
    "            output = x\n",
    "            for layer in network:\n",
    "                output = layer.forward(output)\n",
    "            \n",
    "            # error (display purpose only)\n",
    "            error += cross_entropy(y_onehot, output)\n",
    "            \n",
    "            acc += np.sum(np.argmax(output, axis=1) == y_true)\n",
    "\n",
    "            # backward\n",
    "            output_error = cross_entropy_prime(y_onehot, output)\n",
    "            for layer in reversed(network):\n",
    "                output_error = layer.backward(output_error, learning_rate)\n",
    "        \n",
    "        error /= len(X_train)\n",
    "        acc /= len(X_train)\n",
    "        print('%d/%d, error=%f acc=%f' % (epoch + 1, epochs, error, acc))\n",
    "\n",
    "    # Save the weights\n",
    "    with open('./models/%s_weights.pkl' % dataset_name, 'wb') as f:\n",
    "        pkl.dump(network, f)\n",
    "    \n",
    "    ## END TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u3Pop_HsvuEZ"
   },
   "source": [
    "Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ttYbN2psvtu_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x -- (60000, 28, 28); train_y -- (60000,)\n",
      "train_x -- (2936, 2048); train_y -- (2936,)\n"
     ]
    }
   ],
   "source": [
    "dataset = \"mnist\" \n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_mnist = pkl.load(file)\n",
    "    print(f\"train_x -- {train_mnist[0].shape}; train_y -- {train_mnist[1].shape}\")\n",
    "\n",
    "mx_tr, my_tr, mx_val, my_val = split_data(train_mnist[0], train_mnist[1])\n",
    "fit(mx_tr, my_tr, 'mnist')\n",
    "\n",
    "dataset = \"flowers\" # \"mnist\"/\"flowers\"\n",
    "with open(f\"./data/{dataset}_train.pkl\", \"rb\") as file:\n",
    "    train_flowers = pkl.load(file)\n",
    "    print(f\"train_x -- {train_flowers[0].shape}; train_y -- {train_flowers[1].shape}\")\n",
    "\n",
    "fx_tr, fy_tr, fx_val, fy_val = split_data(train_flowers[0], train_flowers[1]) \n",
    "fit(fx_tr, fy_tr, 'flowers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QprSHht4iwe9"
   },
   "outputs": [],
   "source": [
    "def predict(X_test, dataset_name):\n",
    "    \"\"\"\n",
    "    input.shape[0], input.shape[1]*input.shape[2]\n",
    "    X_test -- np array of share (num_test, 2048) for flowers and (num_test, 28, 28) for mnist.\n",
    "\n",
    "    \n",
    "\n",
    "    This is the only function that we will call from the auto grader. \n",
    "\n",
    "    This function should only perform inference, please donot train your models here.\n",
    "    \n",
    "    Steps to be done here:\n",
    "    1. Load your trained weights from ./models/{dataset_name}_weights.pkl\n",
    "    2. Ensure that you read weights using only the libraries we have given above.\n",
    "    3. Initialize your model with your trained weights\n",
    "    4. Compute the predicted labels and return it\n",
    "\n",
    "    Please provide us the complete code you used for training including any techniques\n",
    "    like data augmentation etc. that you have tried out. \n",
    "\n",
    "    Return:\n",
    "    Y_test - nparray of shape (num_test,)\n",
    "    \"\"\"\n",
    "    Y_test = np.zeros(X_test.shape[0],)\n",
    "\n",
    "    ## TODO\n",
    "\n",
    "    #predict using your trained model\n",
    "    #load your trained weights\n",
    "    with open(\"./models/{}_weights.pkl\".format(dataset_name), \"rb\") as file:\n",
    "        network = pkl.load(file)\n",
    "    \n",
    "    preprocessing(X_test)\n",
    "    error = 0\n",
    "    for i in range(X_test.shape[0]):\n",
    "        output = X_test[i]\n",
    "        for layer in network:\n",
    "            output = layer.forward(output)\n",
    "        \n",
    "        Y_test[i] = np.argmax(output, axis=1)\n",
    "    \n",
    "    ## END TODO\n",
    "    assert Y_test.shape == (X_test.shape[0],) and type(Y_test) == type(X_test), \"Check what you return\"\n",
    "    return Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on MNIST Training set: 0.9425208333333334\n",
      "Accuracy on MNIST Validation set: 0.9394166666666667\n",
      "Accuracy on FLOWERS Training set: 0.9510221465076661\n",
      "Accuracy on FLOWERS Validation set: 0.9472789115646258\n"
     ]
    }
   ],
   "source": [
    "yt = predict(mx_tr, 'mnist')\n",
    "acc = np.mean(yt == my_tr)\n",
    "print(f\"Accuracy on MNIST Training set: {acc}\")\n",
    "\n",
    "yt = predict(mx_val, 'mnist')\n",
    "acc = np.mean(yt == my_val)\n",
    "print(f\"Accuracy on MNIST Validation set: {acc}\")\n",
    "\n",
    "yt = predict(fx_tr, 'flowers')\n",
    "acc = np.mean(yt == fy_tr)\n",
    "print(f\"Accuracy on FLOWERS Training set: {acc}\")\n",
    "\n",
    "yt = predict(fx_val, 'flowers')\n",
    "acc = np.mean(yt == fy_val)\n",
    "print(f\"Accuracy on FLOWERS Validation set: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
